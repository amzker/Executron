{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuG5Dctk30U2"
   },
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMDppGYIsW-x",
    "outputId": "3db0653e-380f-4897-de43-cbdabc5eef55"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "data_filename = \"data/dataset.csv\"\n",
    "word_index_file = \"config/word_to_index.json\"\n",
    "classes_file = \"config/classes.json\"\n",
    "max_token_file = \"config/max_token.txt\"\n",
    "\n",
    "data = pd.read_csv(data_filename)\n",
    "\n",
    "words = set()\n",
    "for text in data[\"X\"]:\n",
    "    words.update(word_tokenize(text.lower()))\n",
    "\n",
    "\n",
    "word_to_index = {word: index + 1 for index, word in enumerate(words)}  # Start index from 1\n",
    "with open(word_index_file, 'w') as json_file:\n",
    "    json.dump(word_to_index, json_file)\n",
    "\n",
    "data[\"X_tokenized\"] = data[\"X\"].apply(lambda text: [word_to_index[word] for word in word_tokenize(text.lower())])\n",
    "\n",
    "X_tokenized = pd.DataFrame({'X_tokenized': data['X_tokenized']})\n",
    "X_tokenized['X_tokenized'] = X_tokenized['X_tokenized'].apply(lambda x: \",\".join(map(str, x)))\n",
    "max_tokens = max(X_tokenized['X_tokenized'].apply(lambda x: len(x.split(','))))\n",
    "\n",
    "\n",
    "with open(max_token_file, 'w') as max_token_file:\n",
    "    max_token_file.write(str(max_tokens))\n",
    "\n",
    "column_names = [f'Token_{i}' for i in range(1, max_tokens + 1)]\n",
    "X_tokenized[column_names] = X_tokenized['X_tokenized'].str.split(',', expand=True)\n",
    "X_tokenized = X_tokenized.drop(columns=['X_tokenized'])\n",
    "\n",
    "X = X_tokenized.fillna(0)\n",
    "for column in  X.columns:\n",
    "    X[column] = pd.to_numeric(X[column])\n",
    "\n",
    "Y = data.drop(columns=[\"X\",\"Y\",\"X_tokenized\"])\n",
    "\n",
    "\n",
    "classes = list(Y.columns)\n",
    "class_mapping = {str(index): label for index, label in enumerate(classes)}\n",
    "with open(classes_file, 'w') as json_file:\n",
    "    json.dump(class_mapping, json_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "am7Vpb2msO03"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "q2mdYPnWQib5"
   },
   "outputs": [],
   "source": [
    "class POLY2:\n",
    "\n",
    "  def __init__(self):\n",
    "      self.beta = None\n",
    "      self.c = None\n",
    "      self.degree = None\n",
    "      self.mean = None\n",
    "      self.std = None\n",
    "      self.mse = []\n",
    "      self.betas = []\n",
    "      self.itr = []\n",
    "\n",
    "\n",
    "  def polyrise(self, X, degree, interactions=True):\n",
    "      newx = np.asarray(X)\n",
    "\n",
    "      if newx.ndim == 1:\n",
    "          newx = newx.reshape(-1, 1)\n",
    "      X_poly = newx.copy()\n",
    "\n",
    "      for i in range(2, degree + 1):\n",
    "          X_poly = np.append(X_poly, newx ** i, axis=1)\n",
    "\n",
    "      if interactions:\n",
    "          for i in range(newx.shape[1]):\n",
    "              for j in range(i + 1, newx.shape[1]):\n",
    "                  interx = newx[:, i] * newx[:, j]\n",
    "                  X_poly = np.append(X_poly, interx.reshape(-1, 1), axis=1)\n",
    "      return X_poly\n",
    "\n",
    "  def normalize(self, X):\n",
    "      smallvalue = 1e-10\n",
    "\n",
    "      X = (X - self.mean) / (self.std + smallvalue)\n",
    "      return X\n",
    "\n",
    "  def fit(self, X, y, lr=0.01, epochs=100, degree=1,interactions=True,alpha=0.01):\n",
    "      self.degree = degree\n",
    "\n",
    "      X_poly = self.polyrise(X, degree, interactions)\n",
    "      y = np.asarray(y)\n",
    "      if y.ndim == 1:\n",
    "        y = y.reshape(-1,1)\n",
    "\n",
    "      n_samples, n_features = X_poly.shape\n",
    "      n_outputs = y.shape[1]\n",
    "      self.beta = np.zeros((n_features, n_outputs))\n",
    "      self.c = np.zeros(n_outputs)\n",
    "      self.mean = np.mean(X_poly, axis=0)\n",
    "      self.std = np.std(X_poly, axis=0)\n",
    "      X_norm = self.normalize(X_poly)\n",
    "\n",
    "      for i in range(epochs):\n",
    "          self.itr.append(i)\n",
    "          pred = X_norm.dot(self.beta) + self.c\n",
    "          error = y - pred\n",
    "          self.betas.append(self.beta)\n",
    "          self.mse.append(np.mean(np.absolute(error)))\n",
    "\n",
    "          #∂β = −2/n Σ X.T(y−βX) +  α∗sign(β)\n",
    "\n",
    "          db = -2 / len(X_norm) * X_norm.T.dot(error)\n",
    "          lasso = alpha * np.sign(self.beta)\n",
    "          db = db + lasso\n",
    "\n",
    "          dc = (-2) * np.mean(error, axis=0)\n",
    "          self.beta = self.beta - (lr * db)\n",
    "          self.c = self.c - (lr * dc)\n",
    "      return self\n",
    "\n",
    "  def predict(self, X):\n",
    "      if self.beta is None or self.c is None:\n",
    "          raise RuntimeError(\"Model has not been trained. Please call model.fit() before model.predict().\")\n",
    "      X_poly = self.polyrise(X, self.degree)\n",
    "      X_norm = self.normalize(X_poly)\n",
    "      return X_norm.dot(self.beta) + self.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_qyP1_eeQ5CB"
   },
   "outputs": [],
   "source": [
    "def plot_graphs(x, y_true, model):\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "  ax1.scatter(x, y_true, color=\"yellow\", label=\"True Data\")\n",
    "  ax1.plot(x, model.predict(x), color=\"red\", label=\"Model Prediction\")\n",
    "  ax1.set_xlabel(\"x\")\n",
    "  ax1.set_ylabel(\"y\")\n",
    "  ax1.set_title(\"True Data vs. our Model Prediction\")\n",
    "  ax1.legend()\n",
    "\n",
    "  equation = f\"y = {model.c}\"\n",
    "  for i, coeff in enumerate(model.beta):\n",
    "      equation += f\" + {coeff} * x^{i+1}\"\n",
    "  print(equation)\n",
    "\n",
    "  ax2.plot(model.itr, model.mse)\n",
    "  ax2.set_xlabel(\"Iterations\")\n",
    "  ax2.set_ylabel(\"MSE\")\n",
    "  ax2.set_title(\"Mean Squared Error our model\")\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "def plot_depth_graph(model):\n",
    "    betas = np.asarray(model.betas)\n",
    "    mse = np.asarray(model.mse)\n",
    "\n",
    "    num_features = betas.shape[1]\n",
    "    num_iterations = betas.shape[0]\n",
    "\n",
    "    num_rows = int(np.ceil(num_features / 2))\n",
    "    fig, axes = plt.subplots(num_rows, 2, figsize=(12, 2 * num_rows))\n",
    "\n",
    "    for i in range(num_features):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        axes[row, col].plot(betas[:, i], mse)\n",
    "        axes[row, col].set_xlabel(f'Beta[{i+1}]')\n",
    "        axes[row, col].set_ylabel('MSE')\n",
    "\n",
    "    fig.suptitle('MSE vs. Beta')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oaSpvxM9QzzO",
    "outputId": "8b067b71-98b1-42c2-c281-51939a16fa80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.POLY2 at 0x7effe4288100>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelpoly = POLY2()\n",
    "\n",
    "modelpoly.fit(X,Y,lr=0.01,epochs=100,degree=2,interactions=True,alpha=1)\n",
    "#plot_depth_graph(modelpoly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "c6L3Erouswl9"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "word_index_file = \"config/word_to_index.json\"\n",
    "classes_file = \"config/classes.json\"\n",
    "max_token_file = \"config/max_token.txt\"\n",
    "\n",
    "def load_word_to_index(filename):\n",
    "    with open(filename, 'r') as json_file:\n",
    "        word_to_index = json.load(json_file)\n",
    "    return word_to_index\n",
    "\n",
    "def load_classes(filename):\n",
    "    with open(filename, 'r') as json_file:\n",
    "        classes = json.load(json_file)\n",
    "    return classes\n",
    "\n",
    "def load_max_token_input_size(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        max_tokens = file.read()\n",
    "    return int(max_tokens)\n",
    "\n",
    "word_to_index = load_word_to_index(word_index_file)\n",
    "max_tokens = load_max_token_input_size(max_token_file)\n",
    "classes = load_classes(classes_file)\n",
    "\n",
    "def make_predictions(input_text, word_to_index, model, max_input_size):\n",
    "    input_tokens = [word_to_index.get(word, 0) for word in word_tokenize(input_text.lower())]\n",
    "    input_tokens = input_tokens[:max_input_size] + [0] * (max_input_size - len(input_tokens))\n",
    "    predicted_output = model.predict(np.array([input_tokens]))\n",
    "\n",
    "    # Create a dictionary of class names and their scores\n",
    "    class_scores = {class_name: score for class_name, score in zip(classes, predicted_output[0])}\n",
    "\n",
    "    return class_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3eo9b4oHYCg",
    "outputId": "6cc58d9c-99f9-4d6a-c650-6aedd6c23aa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0.11001999781798301, '1': 0.15575373293895492, '2': 0.18493434685552582, '3': 0.07795655350292555, '4': 0.09870407011169934, '5': -0.003946788382648225, '6': 0.02032262391868296, '7': 0.057098151706865385, '8': 0.0057987153786529105, '9': 0.1395102081904402, '10': 0.03937794323570284, '11': 0.047295046899982915}\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"take a screenshot\"\n",
    "predictions = make_predictions(input_sentence, word_to_index, modelpoly, max_tokens)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QMLTM4bsw96"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
